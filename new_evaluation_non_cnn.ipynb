{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84881561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "import shutil\n",
    "import tqdm # Import tqdm function directly\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "415e250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== HOG Extraction Starts! ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 480/480 [00:08<00:00, 57.09it/s]\n",
      "100%|██████████| 368/368 [00:07<00:00, 50.33it/s]\n",
      "100%|██████████| 476/476 [00:09<00:00, 51.85it/s]\n",
      "100%|██████████| 368/368 [00:07<00:00, 50.42it/s]\n",
      "100%|██████████| 368/368 [00:07<00:00, 52.32it/s]\n",
      "100%|██████████| 472/472 [00:08<00:00, 55.59it/s]\n",
      "100%|██████████| 364/364 [00:07<00:00, 50.46it/s]\n",
      "100%|██████████| 480/480 [00:09<00:00, 51.52it/s]\n",
      "100%|██████████| 292/292 [00:05<00:00, 57.52it/s]\n",
      "100%|██████████| 368/368 [00:06<00:00, 52.67it/s]\n",
      "100%|██████████| 472/472 [00:09<00:00, 52.17it/s]\n",
      "100%|██████████| 364/364 [00:06<00:00, 52.38it/s]\n",
      "100%|██████████| 364/364 [00:06<00:00, 53.69it/s]\n",
      "100%|██████████| 480/480 [00:08<00:00, 56.16it/s]\n",
      "100%|██████████| 348/348 [00:06<00:00, 57.79it/s]\n",
      "100%|██████████| 480/480 [00:09<00:00, 51.98it/s]\n",
      "100%|██████████| 480/480 [00:12<00:00, 39.80it/s]\n",
      "100%|██████████| 364/364 [00:08<00:00, 40.66it/s]\n",
      "100%|██████████| 480/480 [00:12<00:00, 39.87it/s]\n",
      "100%|██████████| 368/368 [00:09<00:00, 39.51it/s]\n",
      "100%|██████████| 472/472 [00:11<00:00, 40.49it/s]\n",
      "100%|██████████| 472/472 [00:11<00:00, 40.15it/s]\n",
      "100%|██████████| 360/360 [00:09<00:00, 38.57it/s]\n",
      "100%|██████████| 364/364 [00:09<00:00, 36.82it/s]\n",
      "100%|██████████| 368/368 [00:08<00:00, 44.67it/s]\n",
      "100%|██████████| 320/320 [00:08<00:00, 36.99it/s]\n",
      "100%|██████████| 480/480 [00:12<00:00, 39.60it/s]\n",
      "100%|██████████| 340/340 [00:08<00:00, 40.36it/s]\n",
      "100%|██████████| 476/476 [00:12<00:00, 38.68it/s]\n",
      "100%|██████████| 480/480 [00:12<00:00, 39.32it/s]\n",
      "100%|██████████| 368/368 [00:09<00:00, 40.24it/s]\n",
      "100%|██████████| 368/368 [00:09<00:00, 38.53it/s]\n",
      "100%|██████████| 368/368 [00:09<00:00, 40.61it/s]\n",
      "100%|██████████| 476/476 [00:11<00:00, 41.55it/s]\n",
      "100%|██████████| 344/344 [00:09<00:00, 37.96it/s]\n",
      "100%|██████████| 368/368 [00:09<00:00, 37.66it/s]\n",
      "100%|██████████| 480/480 [00:12<00:00, 39.42it/s]\n",
      "100%|██████████| 480/480 [00:11<00:00, 40.19it/s]\n",
      "100%|██████████| 368/368 [00:09<00:00, 39.46it/s]\n",
      "100%|██████████| 472/472 [00:11<00:00, 41.21it/s]\n",
      "100%|██████████| 368/368 [00:08<00:00, 40.99it/s]\n",
      "100%|██████████| 472/472 [00:11<00:00, 39.46it/s]\n",
      "100%|██████████| 472/472 [00:11<00:00, 41.43it/s]\n",
      "100%|██████████| 472/472 [00:11<00:00, 41.30it/s]\n",
      "100%|██████████| 364/364 [00:08<00:00, 41.33it/s]\n",
      "100%|██████████| 476/476 [00:11<00:00, 40.92it/s]\n",
      "100%|██████████| 368/368 [00:06<00:00, 53.55it/s]\n",
      "100%|██████████| 348/348 [00:05<00:00, 63.04it/s]\n",
      "100%|██████████| 472/472 [00:07<00:00, 64.31it/s]\n",
      "100%|██████████| 464/464 [00:09<00:00, 48.54it/s]\n",
      "51it [07:46,  9.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== HOG Extraction Completed! ======\n",
      "(20736, 1764)\n",
      "====== HOG Extraction Starts! ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 39.93it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 42.71it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 42.43it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 41.51it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 42.31it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 56.71it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 61.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 61.41it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 60.81it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 73.08it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 76.16it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 54.63it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 61.52it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 53.98it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 49.69it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 49.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 51.21it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 49.66it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 49.28it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 48.02it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 50.81it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 65.45it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 50.86it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 52.39it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 52.02it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 49.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 48.92it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 54.75it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 52.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 53.62it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 51.19it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 57.89it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 51.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 53.57it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 60.86it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 53.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 62.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 50.63it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 50.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 49.95it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 55.87it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 52.39it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 53.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 53.92it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 52.88it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 55.79it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 53.28it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 56.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 52.24it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 47.64it/s]\n",
      "51it [00:25,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== HOG Extraction Completed! ======\n",
      "(1296, 1764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_hog_features_recursive(input_dir, force_size = (128, 128), pixels_per_cell=(16, 16), cells_per_block=(2, 2)):\n",
    "    features = []\n",
    "    filenames = []\n",
    "    supported_formats = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.webp')\n",
    "    for root, dirs, files in tqdm.tqdm(os.walk(input_dir)):\n",
    "        for filename in tqdm.tqdm(files):\n",
    "            if filename.lower().endswith(supported_formats):\n",
    "                img_path = os.path.join(root, filename)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                # force resized in case feature extraction failed\n",
    "                img_resized = cv2.resize(img, force_size, interpolation=cv2.INTER_AREA)\n",
    "                # pixel normalisation\n",
    "                img_normalised = img_resized.astype(np.float32) / 255.0\n",
    "                # Extract HOG features\n",
    "                try:\n",
    "                    hog_feature = hog(img_normalised, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block, feature_vector=True)\n",
    "                    features.append(hog_feature)\n",
    "                    rel_path = os.path.relpath(img_path, input_dir)\n",
    "                    filenames.append(rel_path)\n",
    "                except Exception as e:\n",
    "                    print(\"WARNING: {img_path} Failed with HOG feature extraction!\")\n",
    "                    continue\n",
    "    hogged = np.array(features)\n",
    "    return hogged, filenames\n",
    "\n",
    "# train set:\n",
    "input_dir = '../CS610_AML_Group_Project/augmented_train_images'\n",
    "print('====== HOG Extraction Starts! ======')\n",
    "hogged_train, filenames_train = extract_hog_features_recursive(input_dir)\n",
    "print('====== HOG Extraction Completed! ======')\n",
    "print(hogged_train.shape)  # (num_images, hog_feature_dim)\n",
    "\n",
    "# test set:\n",
    "input_dir = '../CS610_AML_Group_Project/split_images/test'\n",
    "print('====== HOG Extraction Starts! ======')\n",
    "hogged_test, filenames_test = extract_hog_features_recursive(input_dir)\n",
    "print('====== HOG Extraction Completed! ======')\n",
    "print(hogged_test.shape)  # (num_images, hog_feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "befacc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labeling\n",
    "y_train = [f.split(os.sep)[0] for f in filenames_train]\n",
    "\n",
    "#split data into train_test split\n",
    "x_train = hogged_train.astype(np.float32)\n",
    "y_train = np.array(y_train)\n",
    "y_train, uniques = pd.factorize(y_train)\n",
    "x_train = pd.DataFrame(x_train, dtype=np.float32)\n",
    "y_train = pd.Series(y_train, dtype=np.int32)\n",
    "\n",
    "#Labeling\n",
    "y_test = [f.split(os.sep)[0] for f in filenames_test]\n",
    "\n",
    "#split data into train_test split\n",
    "x_test = hogged_test.astype(np.float32)\n",
    "y_test = np.array(y_test)\n",
    "y_test, uniques = pd.factorize(y_test)\n",
    "x_test = pd.DataFrame(x_test, dtype=np.float32)\n",
    "y_test = pd.Series(y_test, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c275fc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Feature Standardisation Started! ======\n",
      "\n",
      "====== Feature Standardisation Completed! ======\n",
      "The Shape for Training Set after Feature Standardisation: (20736, 1764)\n",
      "The Shape for Testing Set after Feature Standardisation: (1296, 1764)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n====== Feature Standardisation Started! ======\")\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "print(\"\\n====== Feature Standardisation Completed! ======\")\n",
    "print(f\"The Shape for Training Set after Feature Standardisation: {x_train_scaled.shape}\")\n",
    "print(f\"The Shape for Testing Set after Feature Standardisation: {x_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c609844f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Dimensionality Reduction by PCA Started! ======\n",
      "\n",
      "====== Dimensionality Reduction by PCA Completed! ======\n",
      "The Shape for Training Set after Dimensionality Reduction by PCA: (20736, 241)\n",
      "The Shape for Testing Set after Dimensionality Reduction by PCA: (1296, 241)\n",
      "The Number of Chosen PCA: 241\n",
      "The Explained Variance Ratio: 0.8505\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n====== Dimensionality Reduction by PCA Started! ======\")\n",
    "pca = PCA(n_components=0.85, random_state=42)\n",
    "pca.fit(x_train_scaled)\n",
    "\n",
    "\n",
    "x_train_pca = pca.transform(x_train_scaled)\n",
    "x_test_pca = pca.transform(x_test_scaled)\n",
    "\n",
    "print(\"\\n====== Dimensionality Reduction by PCA Completed! ======\")\n",
    "print(f\"The Shape for Training Set after Dimensionality Reduction by PCA: {x_train_pca.shape}\")\n",
    "print(f\"The Shape for Testing Set after Dimensionality Reduction by PCA: {x_test_pca.shape}\")\n",
    "print(f\"The Number of Chosen PCA: {pca.n_components_}\")\n",
    "print(f\"The Explained Variance Ratio: {np.sum(pca.explained_variance_ratio_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54050828",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_bank/best_hog_rf_model.pkl', 'rb') as file:\n",
    "    best_hog_rf = pickle.load(file)\n",
    "with open('model_bank/best_hog_xgb_model.pkl', 'rb') as file:\n",
    "    best_hog_xgb = pickle.load(file)\n",
    "with open('model_bank/best_hog_knn_model.pkl', 'rb') as file:\n",
    "    best_hog_knn = pickle.load(file)\n",
    "with open('model_bank/best_cnn_rf_model.pkl', 'rb') as file:\n",
    "    best_cnn_rf = pickle.load(file)\n",
    "with open('model_bank/best_cnn_xgb_model.pkl', 'rb') as file:\n",
    "    best_cnn_xgb = pickle.load(file)\n",
    "with open('model_bank/best_cnn_knn_model.pkl', 'rb') as file:\n",
    "    best_cnn_knn = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0ec1af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Average Type</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.999035</td>\n",
       "      <td>0.307099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.328319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.998908</td>\n",
       "      <td>0.301281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F0.5-Score</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.313624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Metric Average Type     Train      Test\n",
       "0    Accuracy          N/A  0.999035  0.307099\n",
       "1   Precision        macro  0.998950  0.328319\n",
       "2      Recall        macro  0.998908  0.301281\n",
       "3  F0.5-Score        macro  0.998939  0.313624"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict\n",
    "y_train_pred = best_hog_rf.predict(x_train_pca)\n",
    "y_test_pred = best_hog_rf.predict(x_test_pca)\n",
    "\n",
    "# Function to save metrics\n",
    "metrics = {\"Metric\": [], \"Average Type\": [], \"Train\": [], \"Test\": []}\n",
    "def add_metric(name, avg_type, train_value, test_value):\n",
    "    metrics[\"Metric\"].append(name)\n",
    "    metrics[\"Average Type\"].append(avg_type)\n",
    "    metrics[\"Train\"].append(train_value)\n",
    "    metrics[\"Test\"].append(test_value)\n",
    "\n",
    "# Accuracy\n",
    "add_metric(\"Accuracy\", \"N/A\",\n",
    "           accuracy_score(y_train, y_train_pred),\n",
    "           accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Precision\n",
    "for avg in ['macro']:\n",
    "    add_metric(\"Precision\", avg,\n",
    "               precision_score(y_train, y_train_pred, average=avg),\n",
    "               precision_score(y_test, y_test_pred, average=avg))\n",
    "\n",
    "# Recall\n",
    "for avg in ['macro']:\n",
    "    add_metric(\"Recall\", avg,\n",
    "               recall_score(y_train, y_train_pred, average=avg),\n",
    "               recall_score(y_test, y_test_pred, average=avg))\n",
    "\n",
    "# F0.5-Score\n",
    "beta = 0.5 # mis-labelled sneakers are more costly than missing labels\n",
    "for avg in ['macro']:\n",
    "    add_metric(f\"F{beta}-Score\", avg,\n",
    "               fbeta_score(y_train, y_train_pred, beta=beta, average=avg),\n",
    "               fbeta_score(y_test, y_test_pred, beta=beta, average=avg))\n",
    "\n",
    "# Display metrics\n",
    "hog_rf_metrics = pd.DataFrame(metrics)\n",
    "pd.set_option('display.precision', 6)\n",
    "display(hog_rf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81ab25d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"d:\\Anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"d:\\Anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Average Type</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.368827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.998861</td>\n",
       "      <td>0.379593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.998801</td>\n",
       "      <td>0.367517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F0.5-Score</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.998841</td>\n",
       "      <td>0.371319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Metric Average Type     Train      Test\n",
       "0    Accuracy          N/A  0.998939  0.368827\n",
       "1   Precision        macro  0.998861  0.379593\n",
       "2      Recall        macro  0.998801  0.367517\n",
       "3  F0.5-Score        macro  0.998841  0.371319"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict\n",
    "y_train_pred = best_hog_knn.predict(x_train_pca)\n",
    "y_test_pred = best_hog_knn.predict(x_test_pca)\n",
    "\n",
    "# Function to save metrics\n",
    "metrics = {\"Metric\": [], \"Average Type\": [], \"Train\": [], \"Test\": []}\n",
    "def add_metric(name, avg_type, train_value, test_value):\n",
    "    metrics[\"Metric\"].append(name)\n",
    "    metrics[\"Average Type\"].append(avg_type)\n",
    "    metrics[\"Train\"].append(train_value)\n",
    "    metrics[\"Test\"].append(test_value)\n",
    "\n",
    "# Accuracy\n",
    "add_metric(\"Accuracy\", \"N/A\",\n",
    "           accuracy_score(y_train, y_train_pred),\n",
    "           accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Precision\n",
    "for avg in ['macro']:\n",
    "    add_metric(\"Precision\", avg,\n",
    "               precision_score(y_train, y_train_pred, average=avg),\n",
    "               precision_score(y_test, y_test_pred, average=avg))\n",
    "\n",
    "# Recall\n",
    "for avg in ['macro']:\n",
    "    add_metric(\"Recall\", avg,\n",
    "               recall_score(y_train, y_train_pred, average=avg),\n",
    "               recall_score(y_test, y_test_pred, average=avg))\n",
    "\n",
    "# F0.5-Score\n",
    "beta = 0.5 # mis-labelled sneakers are more costly than missing labels\n",
    "for avg in ['macro']:\n",
    "    add_metric(f\"F{beta}-Score\", avg,\n",
    "               fbeta_score(y_train, y_train_pred, beta=beta, average=avg),\n",
    "               fbeta_score(y_test, y_test_pred, beta=beta, average=avg))\n",
    "\n",
    "# Display metrics\n",
    "hog_knn_metrics = pd.DataFrame(metrics)\n",
    "pd.set_option('display.precision', 6)\n",
    "display(hog_knn_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa5012b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Average Type</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.999035</td>\n",
       "      <td>0.341821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.998918</td>\n",
       "      <td>0.353889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.998934</td>\n",
       "      <td>0.337784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F0.5-Score</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.998919</td>\n",
       "      <td>0.345981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Metric Average Type     Train      Test\n",
       "0    Accuracy          N/A  0.999035  0.341821\n",
       "1   Precision        macro  0.998918  0.353889\n",
       "2      Recall        macro  0.998934  0.337784\n",
       "3  F0.5-Score        macro  0.998919  0.345981"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict\n",
    "y_train_pred = best_hog_xgb.predict(x_train_pca)\n",
    "y_test_pred = best_hog_xgb.predict(x_test_pca)\n",
    "\n",
    "# Function to save metrics\n",
    "metrics = {\"Metric\": [], \"Average Type\": [], \"Train\": [], \"Test\": []}\n",
    "def add_metric(name, avg_type, train_value, test_value):\n",
    "    metrics[\"Metric\"].append(name)\n",
    "    metrics[\"Average Type\"].append(avg_type)\n",
    "    metrics[\"Train\"].append(train_value)\n",
    "    metrics[\"Test\"].append(test_value)\n",
    "\n",
    "# Accuracy\n",
    "add_metric(\"Accuracy\", \"N/A\",\n",
    "           accuracy_score(y_train, y_train_pred),\n",
    "           accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Precision\n",
    "for avg in ['macro']:\n",
    "    add_metric(\"Precision\", avg,\n",
    "               precision_score(y_train, y_train_pred, average=avg),\n",
    "               precision_score(y_test, y_test_pred, average=avg))\n",
    "\n",
    "# Recall\n",
    "for avg in ['macro']:\n",
    "    add_metric(\"Recall\", avg,\n",
    "               recall_score(y_train, y_train_pred, average=avg),\n",
    "               recall_score(y_test, y_test_pred, average=avg))\n",
    "\n",
    "# F0.5-Score\n",
    "beta = 0.5 # mis-labelled sneakers are more costly than missing labels\n",
    "for avg in ['macro']:\n",
    "    add_metric(f\"F{beta}-Score\", avg,\n",
    "               fbeta_score(y_train, y_train_pred, beta=beta, average=avg),\n",
    "               fbeta_score(y_test, y_test_pred, beta=beta, average=avg))\n",
    "\n",
    "# Display metrics\n",
    "hog_xgb_metrics = pd.DataFrame(metrics)\n",
    "pd.set_option('display.precision', 6)\n",
    "display(hog_xgb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee068d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process image data for feature extraction using CNN\n",
    "input_dir = '../CS610_AML_Group_Project/resized_images'\n",
    "img_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])]) #mean and std based on ImageNet - normalise image data closer to normal distribution\n",
    "img_dataset = datasets.ImageFolder(input_dir, transform=img_transform)\n",
    "data_loader = DataLoader(img_dataset, batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a9e6fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for CNN feature extraction\n",
    "def cnn_feature_extract(cnn_feature_extractor, data_loader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    #prepare cnn model to use for feature extraction\n",
    "    cnn_feature_extractor.eval()\n",
    "    cnn_feature_extractor.fc = torch.nn.Identity() #replace fully connected layer of pretrained cnn with Identity layer\n",
    "    for para in cnn_feature_extractor.parameters():\n",
    "        para.requires_grad = False #freeze weights\n",
    "    #feature extraction\n",
    "    features_list, labels_list = [], []\n",
    "    cnn_feature_extractor.to(device)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            feature = cnn_feature_extractor(images)\n",
    "            feature = feature.view(feature.size(0),-1) #flatten into (n_samples, n_features) for non-CNN models\n",
    "            #convert tensors into numpy for fitting into non-CNN models and add into lists\n",
    "            features_list.append(feature.cpu().numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "\n",
    "    return cnn_feature_extractor, np.vstack(features_list), np.hstack(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f21f79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise and extract features using CNN feature extractor\n",
    "weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "resnet50_extractor = models.resnet50(weights=weights)\n",
    "resnet50_extractor, X, y = cnn_feature_extract(resnet50_extractor, data_loader) #X = features, y =labels\n",
    "#no need labelling as the numpy array is generated from the data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "803c601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 5184\n",
      "Number of Labels: 50\n"
     ]
    }
   ],
   "source": [
    "#CNN training and test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)\n",
    "x_train = pd.DataFrame(x_train, dtype=np.float32)\n",
    "y_train = pd.Series(y_train, dtype=np.int32)\n",
    "x_test = pd.DataFrame(x_test, dtype=np.float32)\n",
    "y_test = pd.Series(y_test, dtype=np.int32)\n",
    "#same as original flow\n",
    "print(\"Number of Samples:\", len(y_train))\n",
    "print(\"Number of Labels:\", len(np.unique(y_train)))\n",
    "counts = y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc77b305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Average Type</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.998071</td>\n",
       "      <td>0.368827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.997978</td>\n",
       "      <td>0.391356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.997817</td>\n",
       "      <td>0.357844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F0.5-Score</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.997926</td>\n",
       "      <td>0.356892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Metric Average Type     Train      Test\n",
       "0    Accuracy          N/A  0.998071  0.368827\n",
       "1   Precision        macro  0.997978  0.391356\n",
       "2      Recall        macro  0.997817  0.357844\n",
       "3  F0.5-Score        macro  0.997926  0.356892"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict\n",
    "y_train_pred = best_cnn_rf.predict(x_train)\n",
    "y_test_pred = best_cnn_rf.predict(x_test)\n",
    "\n",
    "# Function to save metrics\n",
    "metrics = {\"Metric\": [], \"Average Type\": [], \"Train\": [], \"Test\": []}\n",
    "def add_metric(name, avg_type, train_value, test_value):\n",
    "    metrics[\"Metric\"].append(name)\n",
    "    metrics[\"Average Type\"].append(avg_type)\n",
    "    metrics[\"Train\"].append(train_value)\n",
    "    metrics[\"Test\"].append(test_value)\n",
    "\n",
    "# Accuracy\n",
    "add_metric(\"Accuracy\", \"N/A\",\n",
    "           accuracy_score(y_train, y_train_pred),\n",
    "           accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Precision\n",
    "for avg in ['macro']:\n",
    "    add_metric(\"Precision\", avg,\n",
    "               precision_score(y_train, y_train_pred, average=avg),\n",
    "               precision_score(y_test, y_test_pred, average=avg))\n",
    "\n",
    "# Recall\n",
    "for avg in ['macro']:\n",
    "    add_metric(\"Recall\", avg,\n",
    "               recall_score(y_train, y_train_pred, average=avg),\n",
    "               recall_score(y_test, y_test_pred, average=avg))\n",
    "\n",
    "# F0.5-Score\n",
    "beta = 0.5 # mis-labelled sneakers are more costly than missing labels\n",
    "for avg in ['macro']:\n",
    "    add_metric(f\"F{beta}-Score\", avg,\n",
    "               fbeta_score(y_train, y_train_pred, beta=beta, average=avg),\n",
    "               fbeta_score(y_test, y_test_pred, beta=beta, average=avg))\n",
    "\n",
    "# Display metrics\n",
    "cnn_rf_metrics = pd.DataFrame(metrics)\n",
    "pd.set_option('display.precision', 6)\n",
    "display(cnn_rf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7a40d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Average Type</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.998071</td>\n",
       "      <td>0.367284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.997988</td>\n",
       "      <td>0.389494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.997819</td>\n",
       "      <td>0.361929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F0.5-Score</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.997929</td>\n",
       "      <td>0.377110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Metric Average Type     Train      Test\n",
       "0    Accuracy          N/A  0.998071  0.367284\n",
       "1   Precision        macro  0.997988  0.389494\n",
       "2      Recall        macro  0.997819  0.361929\n",
       "3  F0.5-Score        macro  0.997929  0.377110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict\n",
    "y_train_pred = best_cnn_knn.predict(x_train)\n",
    "y_test_pred = best_cnn_knn.predict(x_test)\n",
    "\n",
    "# Function to save metrics\n",
    "metrics = {\"Metric\": [], \"Average Type\": [], \"Train\": [], \"Test\": []}\n",
    "def add_metric(name, avg_type, train_value, test_value):\n",
    "    metrics[\"Metric\"].append(name)\n",
    "    metrics[\"Average Type\"].append(avg_type)\n",
    "    metrics[\"Train\"].append(train_value)\n",
    "    metrics[\"Test\"].append(test_value)\n",
    "\n",
    "# Accuracy\n",
    "add_metric(\"Accuracy\", \"N/A\",\n",
    "           accuracy_score(y_train, y_train_pred),\n",
    "           accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Precision\n",
    "for avg in ['macro']:\n",
    "    add_metric(\"Precision\", avg,\n",
    "               precision_score(y_train, y_train_pred, average=avg),\n",
    "               precision_score(y_test, y_test_pred, average=avg))\n",
    "\n",
    "# Recall\n",
    "for avg in ['macro']:\n",
    "    add_metric(\"Recall\", avg,\n",
    "               recall_score(y_train, y_train_pred, average=avg),\n",
    "               recall_score(y_test, y_test_pred, average=avg))\n",
    "\n",
    "# F0.5-Score\n",
    "beta = 0.5 # mis-labelled sneakers are more costly than missing labels\n",
    "for avg in ['macro']:\n",
    "    add_metric(f\"F{beta}-Score\", avg,\n",
    "               fbeta_score(y_train, y_train_pred, beta=beta, average=avg),\n",
    "               fbeta_score(y_test, y_test_pred, beta=beta, average=avg))\n",
    "\n",
    "# Display metrics\n",
    "cnn_knn_metrics = pd.DataFrame(metrics)\n",
    "pd.set_option('display.precision', 6)\n",
    "display(cnn_knn_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af4949fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Average Type</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.997492</td>\n",
       "      <td>0.484568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.997335</td>\n",
       "      <td>0.486872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.997212</td>\n",
       "      <td>0.479769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F0.5-Score</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.997285</td>\n",
       "      <td>0.480678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Metric Average Type     Train      Test\n",
       "0    Accuracy          N/A  0.997492  0.484568\n",
       "1   Precision        macro  0.997335  0.486872\n",
       "2      Recall        macro  0.997212  0.479769\n",
       "3  F0.5-Score        macro  0.997285  0.480678"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict\n",
    "y_train_pred = best_cnn_xgb.predict(x_train)\n",
    "y_test_pred = best_cnn_xgb.predict(x_test)\n",
    "\n",
    "# Function to save metrics\n",
    "metrics = {\"Metric\": [], \"Average Type\": [], \"Train\": [], \"Test\": []}\n",
    "def add_metric(name, avg_type, train_value, test_value):\n",
    "    metrics[\"Metric\"].append(name)\n",
    "    metrics[\"Average Type\"].append(avg_type)\n",
    "    metrics[\"Train\"].append(train_value)\n",
    "    metrics[\"Test\"].append(test_value)\n",
    "\n",
    "\n",
    "# Accuracy\n",
    "add_metric(\"Accuracy\", \"N/A\",\n",
    "           accuracy_score(y_train, y_train_pred),\n",
    "           accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Precision\n",
    "for avg in ['macro']:\n",
    "    add_metric(\"Precision\", avg,\n",
    "               precision_score(y_train, y_train_pred, average=avg),\n",
    "               precision_score(y_test, y_test_pred, average=avg))\n",
    "\n",
    "# Recall\n",
    "for avg in ['macro']:\n",
    "    add_metric(\"Recall\", avg,\n",
    "               recall_score(y_train, y_train_pred, average=avg),\n",
    "               recall_score(y_test, y_test_pred, average=avg))\n",
    "\n",
    "# F0.5-Score\n",
    "beta = 0.5 # mis-labelled sneakers are more costly than missing labels\n",
    "for avg in ['macro']:\n",
    "    add_metric(f\"F{beta}-Score\", avg,\n",
    "               fbeta_score(y_train, y_train_pred, beta=beta, average=avg),\n",
    "               fbeta_score(y_test, y_test_pred, beta=beta, average=avg))\n",
    "\n",
    "# Display metrics\n",
    "cnn_xgb_metrics = pd.DataFrame(metrics)\n",
    "pd.set_option('display.precision', 6)\n",
    "display(cnn_xgb_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743badc5",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d94f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    }
   ],
   "source": [
    "#check if cuda is available to use\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device, \"is used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85cb3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process image data for feature extraction using CNN\n",
    "input_dir = '../CS610_AML_Group_Project/resized_images'\n",
    "full_set = datasets.ImageFolder(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faefa422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing done\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees = 15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast = 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],std=[0.229,0.224,0.225]) #ImageNet\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize([0.485,0.456,0.406],std=[0.229,0.224,0.225]) #ImageNet\n",
    "])\n",
    "\n",
    "train_size = int(0.7*len(full_set))\n",
    "val_size = int(0.2*len(full_set))\n",
    "test_size = len(full_set)-train_size-val_size\n",
    "split_datasets = random_split(\n",
    "    full_set,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_indices = split_datasets[0].indices\n",
    "val_indices = split_datasets[1].indices\n",
    "test_indices = split_datasets[2].indices\n",
    "\n",
    "class CustomSubsetWithTransform(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = self.indices[idx]\n",
    "        img, label = self.dataset[original_idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "train_img_dataset = CustomSubsetWithTransform(full_set, train_indices, train_transform)\n",
    "val_img_dataset = CustomSubsetWithTransform(full_set, val_indices, val_test_transform)\n",
    "test_img_dataset = CustomSubsetWithTransform(full_set, test_indices, val_test_transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_img_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_img_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_img_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "print(\"Data processing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e32fb0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes (Full Set): 50 \n",
      " ['adidas_forum_high', 'adidas_forum_low', 'adidas_gazelle', 'adidas_nmd_r1', 'adidas_samba', 'adidas_stan_smith', 'adidas_superstar', 'adidas_ultraboost', 'asics_gel-lyte_iii', 'converse_chuck_70_high', 'converse_chuck_70_low', 'converse_chuck_taylor_all-star_high', 'converse_chuck_taylor_all-star_low', 'converse_one_star', 'new_balance_327', 'new_balance_550', 'new_balance_574', 'new_balance_990', 'new_balance_992', 'nike_air_force_1_high', 'nike_air_force_1_low', 'nike_air_force_1_mid', 'nike_air_jordan_11', 'nike_air_jordan_1_high', 'nike_air_jordan_1_low', 'nike_air_jordan_3', 'nike_air_jordan_4', 'nike_air_max_1', 'nike_air_max_270', 'nike_air_max_90', 'nike_air_max_95', 'nike_air_max_97', 'nike_air_max_plus_(tn)', 'nike_air_vapormax_flyknit', 'nike_air_vapormax_plus', 'nike_blazer_mid_77', 'nike_cortez', 'nike_dunk_high', 'nike_dunk_low', 'puma_suede_classic', 'reebok_classic_leather', 'reebok_club_c_85', 'salomon_xt-6', 'vans_authentic', 'vans_old_skool', 'vans_sk8-hi', 'vans_slip-on_checkerboard', 'yeezy_700_wave_runner', 'yeezy_boost_350_v2', 'yeezy_slide']\n"
     ]
    }
   ],
   "source": [
    "#get classes from directory\n",
    "num_classes = len(full_set.classes)\n",
    "class_names = full_set.classes\n",
    "print(\"Number of classes (Full Set):\", num_classes,\"\\n\", full_set.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1cbfafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best cnn_model\n",
    "best_model = torch.load(\"./model_bank/best_cnn_resnet50.pth\")\n",
    "cnn_model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e53d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best cnn_model\n",
    "best_model = torch.load(\"./model_bank/best_cnn_resnet50.pth\")\n",
    "cnn_model = models.resnet50(pretrained=True)\n",
    "cnn_model.fc = nn.Linear(cnn_model.fc.in_features, len(class_names))\n",
    "cnn_model = cnn_model.to(device)\n",
    "cnn_model.load_state_dict(best_model)\n",
    "#create dictionary to store metrics\n",
    "test_holder = {}\n",
    "test_holder['y_true'], test_holder['y_hat'] = [], []\n",
    "#start evaluation of model\n",
    "cnn_model.eval()\n",
    "test_corrects = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = cnn_model(images)\n",
    "        _, test_preds = torch.max(output, 1)\n",
    "        test_corrects += (test_preds == labels).sum().item()\n",
    "        test_holder['y_true'].extend(list(labels.cpu().detach().numpy()))\n",
    "        test_holder['y_hat'].extend(list(test_preds.cpu().detach().numpy()))\n",
    "\n",
    "\n",
    "test_y_true_all = test_holder['y_true']\n",
    "test_y_pred_all = test_holder['y_hat']\n",
    "test_acc = test_corrects / len(test_loader.dataset)\n",
    "test_precision = precision_score(test_y_true_all, test_y_pred_all, average='macro')\n",
    "test_recall = recall_score(test_y_true_all, test_y_pred_all, average='macro')\n",
    "test_fbeta = fbeta_score(test_y_true_all, test_y_pred_all, beta=beta, average='macro')\n",
    "test_class_report = classification_report(test_y_true_all, test_y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34773f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
